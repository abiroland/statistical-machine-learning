---
title: "STAT-427/627 Homework 6 Cross-Validation"
subtitle: "Name: Roland Abi Course: STA 627"
number-sections: true
format:
  pdf:
    toc: false
    toc-depth: 1
    embed-resources: true
---

# Predicting a Grade

Do by hand.
A student wants to predict their final grade for the Statistical Machine Learning course using the KNN algorithm with $K = 3$.
Seven friends who took the course last year had the following mid-term test scores and grades.

a\.
Estimate the prediction error rate of the algorithm, using the validation-set method, with Friends 2, 3, 4, and 6 as training and Friends 1, 5, 7 as testing data.

Distances was obtained using Euclidean Distance: $\sqrt{\sum_{i=1}^{n}(X_{i} - X_{j})^2}$

| Friend          |   1   |  2  |  3  |  4  |   5   |  6  |   7   | $\hat{Y}$ |
|-----------------|:-----:|:---:|:---:|:---:|:-----:|:---:|:-----:|:---------:|
| Midterm         |  90   | 88  | 83  | 78  |  85   | 89  |  93   |           |
| Final Grade     | **A** |  A  |  B  |  B  | **A** |  B  | **A** |           |
| Test 1: 90 Dist |   0   |  2  |  7  | 12  |   5   |  1  |   3   |           |
| Test 1          |   x   |  A  |  x  |  x  |   x   |  B  |   A   |     A     |
| Test 5: 85 Dist |   5   |  3  |  2  |  7  |   0   |  4  |   8   |           |
| Test 5          |   x   |  A  |  B  |  x  |   x   |  B  |   x   |     B     |
| Test 7: 93 Dist |   3   |  5  | 10  | 15  |   8   |  4  |   0   |           |
| Test 7          |   A   |  A  |  x  |  x  |   x   |  B  |   x   |     A     |

Error classification rate 1/3 = 0.33

\newpage

b\.
Estimate the prediction error rate of the algorithm using the leave-one-out cross-validation method.

LOOCV

|             |            |            |     |     |     |     |     |           |
|-------------|:----------:|:----------:|:---:|:---:|:---:|:---:|:---:|:---------:|
| Friend      |     1      |     2      |  3  |  4  |  5  |  6  |  7  |           |
| Midterm     |     90     |     88     | 83  | 78  | 85  | 89  | 93  |           |
| Final Grade |     A      |     A      |  B  |  B  |  A  |  B  |  A  | $\hat{Y}$ |
| Dist - 1    |     \-     |     2      |  7  | 12  |  5  |  1  |  3  |           |
| Pred - 1    |     \-     |     A      |  x  |  x  |  x  |  B  |  A  |     A     |
| Dist - 2    |     2      |     \-     |  5  | 10  |  3  |  1  |  5  |           |
| Pred - 2    |     A      |     \-     |  x  |  x  |  A  |  B  |  x  |     A     |
| Dist - 3    |     7      |     5      | \-  |  5  |  2  |  6  | 10  |           |
| Pred - 3    |     x      |     A      | \-  |  B  |  A  |  x  |  x  |     A     |
| Dist - 4    |     12     |     10     |  5  | \-  |  7  | 11  | 15  |           |
| Pred - 4    |     x      |     A      |  B  | \-  |  A  |  x  |  x  |     A     |
| Dist - 5    |     5      |     3      |  2  |  7  | \-  |  4  |  8  |           |
| Pred - 5    |     x      |     A      |  B  |  x  | \-  |  B  |  x  |     B     |
| Dist - 6    |     1      |     1      |  6  | 11  |  4  | \-  |  4  |           |
| Pred - 6    |     A      |     A      |  x  |  x  | A/2 | \-  | A/2 |     A     |
| Dist - 7    |     3      |     5      | 10  | 15  |  8  |  4  | \-  |           |
| Pred - 7    |     A      |     A      |  x  |  x  |  x  |  B  | \-  |     A     |
| Prediction  | Error Rate | 4/7 = 0.57 |     |     |     |     |     |           |

\newpage

# Cross-Validation on Simulated Data

[(Pages 222-3, chap. 5, #8)]{style="font-size: small;"}

a\.
Generate a simulated data set as follows:

`set.seed(14); x <- rnorm (100); y <- x - 4*x^2 + rnorm(100);`

```{r}
#| label: cv-sim-make-data
#| echo: fenced
#| include: true
#| message: false
library(tidyverse)
library(boot)
set.seed(14)
x <- rnorm(100)
y <- x - 4*x^2 + rnorm(100)
my_data <- data.frame(x,y)

head(my_data)
```

Write out the model used to generate the data in equation form.

$y = x - 4*x^2 + \text{rnom(100)}$

Plot the data and interpret the plot.

```{r}
#| message: false
ggplot(my_data, aes(x = x, y = y))+
  geom_point() +
  geom_smooth(se=FALSE) +
  theme_bw()
```

I have fit a least squares linear model (with a quadratic term forÂ $x$ to the data, and we can see that the fitted model fits the data (and what we would expect from the underlying relationship) very well.

(b) Compute the adjusted LOOCV estimates of prediction error that result from fitting each of the following four regression models:

$Y = \beta_0 + \beta_1 X + \epsilon$

$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon$

$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 +\epsilon$

$Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \beta_3 X^3 + \beta_4 X^4 + \epsilon$

You can use the following data frame or use the `I()` function in the general linear model function formula.

```{r}
#| echo: fenced
#| include: true
my_df <- data.frame(
  x =  x,
  x2 = x^2,
  x3 = x^3,
  x4 = x^4,
  y = y
)
```

(c) Which of these models have the smallest adjusted prediction mean squared error as estimated by LOOCV? Is this what you expected? Explain your answer.

```{r}
#| layout-ncol: 2
#| layout-align: center

library(broom)

cv.error  <- rep (0, 4) # initialize the cross validation error vector cv.error 
glm.fit   <- list()     # initialize the glm.fit list
powers    <- 1:4
# xpoly     <- function(i) poly(x, i, raw = T)
for (i in powers){
  glm.fit[[i]] <- glm(y ~ poly(x, i, raw = T), data = my_data)
  cv.error[i] <- cv.glm(my_data, glm.fit[[i]])$delta[1]
  cat("Fitting model number", i, "\n")
  print(tidy(glm.fit[[i]]))
  cat("------------------------------ \n\n")
}
```

```{r}
min(cv.error)
which.min(cv.error)
```

Model 2 has the smallest adjusted prediction mean squared error as estimated LOOCV.
Yes this was expected, considering the shape of the plot.
In addition the equation for generating the dataset is of order 2

<!-- -->

d.  Use one call to `anova()` to compare all four models using the proper `test=` value and interpret the results in terms of the $p$-value.

```{r}
anova(glm.fit[[1]], glm.fit[[2]], glm.fit[[3]], glm.fit[[4]], test = "Chisq")
```

-   The low p-value (\<2e-16) suggests that order two model is better than high order models.

-   The high p-value (.7482) suggests there is no evidence order three model is better than order 2 model.
    Similar conclusion was reached for order 4 models.

d\.
Repeat step b but with $K=10$-Fold validation and compare the prediction mean squared error.
What do you notice compared to LOOCV?

```{r}
#| layout-ncol: 2
#| layout-align: center

cv.errorkfold  <- rep (0, 4) # initialize the cross validation error vector cv.error 
glm.fitkfold   <- list()     # initialize the glm.fit list
powers    <- 1:4
# xpoly     <- function(i) poly(x, i, raw = T)
for (i in powers){
  glm.fitkfold[[i]] <- glm(y ~ poly(x, i, raw = T), data = my_data)
  cv.errorkfold[i] <- cv.glm(my_data, glm.fitkfold[[i]], K=10)$delta[1]
  cat("Fitting model number", i, "\n")
  print(tidy(glm.fitkfold[[i]]))
  cat("------------------------------ \n\n")
}
```

```{r}
min(cv.errorkfold)
which.min(cv.errorkfold)
```

We observed that the prediction mean squared error for the 10-Fold is smallest for order two model as observed in the LOOCV.
However the error is lower on average for KFold approach (1.022) as compared to the LOOCV approach (1.027).

d.  Use a seed of 14 where appropriate.

```{r}

#| layout-ncol: 2
#| layout-align: center

set.seed(14)

cv.errorseed  <- rep (0, 4) # initialize the cross validation error vector cv.error 
glm.fitseed   <- list()     # initialize the glm.fit list
powers    <- 1:4
# xpoly     <- function(i) poly(x, i, raw = T)
for (i in powers){
  glm.fitseed[[i]] <- glm(y ~ poly(x, i, raw = T), data = my_data)
  cv.errorseed[i] <- cv.glm(my_data, glm.fitseed[[i]], K=10)$delta[1]
  cat("Fitting model number", i, "\n")
  print(tidy(glm.fitseed[[i]]))
  cat("------------------------------ \n\n")
}
```

```{r}
which.min(cv.errorseed)
min(cv.errorseed)
```

Using the seed further decreased the mean prediction error with order two model having the smallest error rate.

# Predicting Defaults on Loans [(Pages 220-221, chap. 5, #5)]{style="font-size: small;"}

Load the 'Default" data set from the {ISLR2} package.

```{r}
#| label: Default-set-up
#| message: false
#| echo: fenced
#| include: true
library(tidyverse)
library(ISLR2)
data("Default")
```

Use logistic regression to create a full model for predicting the probability of default based on predictors `income`, `balance`, and `student`.

```{r}
# Logistic regression full model 1
logreg1 <- glm(default ~ income + balance + student, 
               data = Default, family = "binomial")
summary(logreg1)

# Logistic regression Reduced model 1
logreg2 <- glm(default ~ income + balance, 
               data = Default, family = "binomial")
summary(logreg2)
```

Use each of the following methods to estimate the *prediction error rate* of the full model and then the *prediction error rate* of a reduced model without `student`.

a.  The validation set approach. Use a 60% split for training data, a seed of 123, and a threshold of .5 where appropriate.

```{r}
set.seed(123)

training_pct <- .60 
Z <- sample(nrow(Default), training_pct*nrow(Default))

traindf <- Default[Z, ]
testdf <- Default[-Z, ]
```

```{r}
#Logistic regression Full Model
logitfm <- glm(default ~ income + balance + student,
                data = traindf, 
                family = "binomial")
summary(logitfm)

# Predictions
predsfm <- predict(logitfm, newdata = testdf, type = "response")

# Convert to classification prediction
Yhatfm <-  ifelse(predsfm >= 0.5, "Yes", "No")

# confusion matrix
confmatfm <- table(Yhatfm, testdf$default)
confmatfm

# Correct classification prediction rate
accuracyfm <- sum(confmatfm[1], confmatfm[4])/sum(confmatfm)
accuracyfm
```

```{r}
#Logistic regression Reduced Model
logitrd <- glm(default ~ income + balance,
                data = traindf, 
                family = "binomial")
summary(logitrd)

# Predictions
predsrd <- predict(logitrd, newdata = testdf, type = "response")

# Convert to classification prediction
Yhatrd <-  ifelse(predsrd >= 0.5, "Yes", "No")

# confusion matrix
confmatrd <- table(Yhatrd, testdf$default)
confmatrd

# Correct classification prediction rate
accuracyrd <- sum(confmatrd[1], confmatrd[4])/sum(confmatrd)
accuracyrd
```

a.  Leave-one-out cross-validation. Use a seed of 123 and a threshold of .5 for the loss function.

```{r}
#| cache: true

set.seed(123)

# Loss function
Lossfn <- function(Y, p) {
  mean(1 * (Y == 1 & p <= .50) | (1 * (Y == 0 & p > .50)),
       na.rm = TRUE)
}

# Convert response to numeric 0's and 1's
Default$Y <- as.numeric(as.factor(Default$default)) - 1

# LOOCV_full model 
Loocvfm <- cv.glm(Default, logreg1, cost = Lossfn)$delta
Loocvfm

# LOOCV_reduced model
Loocvrd <- cv.glm(Default, logreg2, cost = Lossfn)$delta
Loocvrd
```

-   With 10,000 observations this will run for a while (mine took 5.9 minutes). Suggest including a chunk option for cache, e.g., `#\| cache: true`, so it will save the results and won't run as long when you render.

c.  $K$-fold cross-validation for $K = 10$ and $K = 100$. Use a seed of 123, and the same loss function with a threshold of .5 where appropriate.

```{r}
#| cache: true

set.seed(123)
# K = 10

# KFold_full model 
Kfoldfm10 <- cv.glm(Default, logreg1, cost = Lossfn, K=10)$delta
Kfoldfm10

# KFold_reduced model
Kfoldrd10 <- cv.glm(Default, logreg2, cost = Lossfn, K=10)$delta
Kfoldrd10
```

```{r}
#| cache: true

set.seed(123)
# K = 100

# KFold_full model 
Kfoldfm100 <- cv.glm(Default, logreg1, cost = Lossfn, K=100)$delta
Kfoldfm100

# KFold_reduced model
Kfoldrd100 <- cv.glm(Default, logreg2, cost = Lossfn, K=100)$delta
Kfoldrd100
```

Summarize the results.

-   Compare the validation set models without and with `student` using `anova()` with the appropriate `test=`.

```{r}
# compare results
anova(logitrd, logitfm, test = "Chisq")
```

The high p-value (.0683) suggests there is no evidence the larger (full) model is any better than the reduced without the two variables.

-   Interpret the results of the different methods/models and the anova test and recommend the better model.

**Validation Method:** Using this approach, the full and reduced logistics regression models accurately predicts that 97% of the time people are likely going to default, based on the observed characteristics respectively.

**LOOCV Method:** For this approach, the prediction error rate and adjusted prediction error rate was (0.02620, 0.02630) respectively for the reduced model while the prediction error rate and the adjusted prediction error rate (0.02680) for the full model were similarly.
Overall, the prediction error rate and adjusted error rate for the reduced model was lower than for the full model.

**KFold Method:** Results obtained from the 10-Fold method showed that prediction error rate and adjusted prediction error for the reduced logistic regression model was lower than the full model.
Similar results were also obtained for the 100-Fold approach.

Giving the results obtained and from this approaches and the results of model comparison we lack sufficient evidence (p = 0.06835) to conclude that logistic regression (full) model with parameters (`Income`, `Balance`, and `Student`) is better than the logistic regression (reduced) model with parameters (`income`, `balance`).
Consequently, we will recommend the reduced logistic regression model as the better model.
