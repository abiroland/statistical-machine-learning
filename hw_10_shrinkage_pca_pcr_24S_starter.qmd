---
title: "STAT-427/627 Homework PCA PCR"
subtitle: "Name:Roland Abi Course: STA 627"
number-sections: true

format:
  html:
    toc: false
    embed-resources: true
---

# College Applications Continued

This continues the College data problem from the previous h/w.
(Chap. 6, \# 9 cd, p.286-287)

Predict the number of applications received based on the other variables in the College data set from {ISLR2}.

```{r}
#| message: false
#| echo: fenced
#| include: true
library(tidyverse)
library(ISLR2)
library(pls)
glimpse(College)
```

Previous results for predicting the number of college applications, using a 50% split validation set and seed 123, include:

::: table-striped
| Method                              | Prediction MSE | Predictors |
|-------------------------------------|---------------:|:----------:|
| Stepwise Least Squares (validation) |      1,327,026 |     12     |
| Stepwise Least Squares (K=10-fold)  |      1,264,962 |     12     |
| Ridge Regression (lambda.min)       |      2,092,402 |     17     |
| Lasso (lambda.1se)                  |      1,528,732 |     4      |
| Lasso (lambda.min)                  |      1,391,186 |     12     |
:::

Now fit models using two additional regularization methods and evaluate performance using the validation set based on 50% split so we remain consistent with previous work.
Use `set.seed(123)` where appropriate:

```{r}
set.seed(123)

training_pct <- .50 

Z <- sample(nrow(College), training_pct*nrow(College))

college_train <- College[Z, ] 
college_test <- College[-Z, ] 
```

\(a\) PCR model.
Choose $M$, the number of principal components, using cross-validation.

```{r}
set.seed(123) # for the cross validation sampling
pcr_reg_cv <- pcr(Apps~ ., data = college_train, scale = TRUE,
               validation = "CV")
summary(pcr_reg_cv)
```

Selecting $M=17$ components because it has the minimum RMSEP (1024)

```{r}
pcr_reg <- pcr(Apps~ ., data = college_train, 
               scale = TRUE, ncomp = 17)
```

\(b\) PLS model.
Choose $M$, the number of principal components, using cross-validation.

```{r}
set.seed(123) # for the cross validation sampling
plsr_reg_cv <- plsr(Apps~ ., data = college_train, scale = TRUE,
               validation = "CV")
summary(plsr_reg_cv)
```

Selecting $M=10$ PCs because it explains approximately $92$% of the variance in `Apps` and also has the minimum RMSEP (1023)

```{r}
plsr_reg <- plsr(Apps~ ., data = college_train, scale = TRUE,
               ncomp = 10)
```

\(c\) Evaluate performance of each method in terms of **prediction MSE** and number of components.
Show the validation plots for both models.

```{r}
# Principal Component Regression prediction mean square error
msep_pcr <- MSEP(object = pcr_reg, estimate = "test",
     newdata = college_test, ncomp = 17)
msep_pcr$val[2]

validationplot(object = pcr_reg, val.type = "MSEP")
```

```{r}
# Partial Least Squares Regression prediction mean square error
msep_plsr <- MSEP(object = plsr_reg, estimate = "test",
     newdata = college_test, ncomp = 10)
msep_plsr$val[2]

validationplot(object = plsr_reg, val.type = "MSEP")
```

\(d\) Add the results into the summary table.
Comment on the results obtained including MSE Prediction and number of predictors.

-   Recommend a method and provide a rationale?

-   Hint: Consider help for: `pls::validationplot()`, `pls::R2()`, `pls::MSEP()`

| Method                              | Prediction MSE | Predictors |
|-------------------------------------|---------------:|:----------:|
| Stepwise Least Squares (validation) |      1,327,026 |     12     |
| Stepwise Least Squares (K=10-fold)  |      1,264,962 |     12     |
| Ridge Regression (lambda.min)       |      2,092,402 |     17     |
| Lasso (lambda.1se)                  |      1,528,732 |     4      |
| Lasso (lambda.min)                  |      1,391,186 |     12     |
| PCR                                 |      1,373,995 |     17     |
| PLS                                 |      1,384,151 |     10     |

The rationale for our model selection, will be model with the minimum prediction mean square error.
The Lasson (lambda.1se) model despite having the minimum number of predictors has the second highest prediction mean square error.
Therefore, we will be recommending the Stepwise Least Square (K-10-fold) model due to having the minimum prediction mean square error (1,264,962).

# Comparison of dimension reduction methods

We want to predict median value of owner-occupied homes, `medv`, in the Boston data set in the {ISLR2} package.
It should have 13 variables.
Leave `rad` as an integer.

```{r}
#| message: false
library(ISLR2)
library(boot)
library(leaps)
library(glmnet)
library(pls)
dplyr::glimpse(Boston)
```

(a) Use four regularization methods, lasso, ridge regression, PCR, and PLS. Recommend a model Provide a rationale based on their predictive performance (Mean Squared Error) and the number of predictors.

-   Use `set.seed(1234)` whenever appropriate.

    ```{r}
    # data preparation
    x <- model.matrix(lm(medv ~ ., data = Boston))[,-1]
    y <- Boston$medv
    ```

-   **Do not use a validation set.** Use k=10 fold cross-validation for all models.

-   a.1 Ridge Regression

    ```{r}
    set.seed(1234)
    rr <- cv.glmnet(x, y, alpha = 0)
    pred_rr <- predict(rr, newx = x, s = "lambda.min")
    pred_MSE_rr <- round(mean((pred_rr - y)^2), digits = 2)
    pred_MSE_rr
    ```

-   a.2 Lasso Regression

    ```{r}
    set.seed(1234)
    lr <- cv.glmnet(x, y)

    pred_lr_min <- predict(lr, newx = x, s = "lambda.min")
    pred_MSE_lr_min <- round(mean((pred_lr_min - y)^2), digits = 2)
    pred_MSE_lr_min

    pred_lr_1se <- predict(lr, newx = x, s = "lambda.1se")
    pred_MSE_lr_1se <- round(mean((pred_lr_1se - y)^2), digits = 2)
    pred_MSE_lr_1se
    ```

-   a.3 PC Regression

    ```{r}
    set.seed(1234) 
    pcr2_reg_cv <- pcr(medv ~ ., data = Boston, scale = TRUE,
                   validation = "CV")

    pcr2 <- pcr(medv ~ ., data = Boston, scale = TRUE,
                ncomp = 12)

    msep_pcr2 <- MSEP(object = pcr2,
         newdata = Boston, ncomp = 12)
    msep_pcr2$val[2]
    ```

-   a.4 PLS Regression

    ```{r}
    set.seed(1234) 
    plsr2_reg_cv <- plsr(medv~ ., data = Boston, scale = TRUE,
                   validation = "CV")
    summary(plsr2_reg_cv)

    plsr2 <- plsr(medv ~ ., data = Boston, scale = TRUE,
                ncomp = 8)

    msep_plsr2 <- MSEP(object = plsr2,
         newdata = Boston, ncomp = 8)
    msep_plsr2$val[2]
    ```

-   a.5 Summary

| Method                        | Prediction MSE | Predictors |
|-------------------------------|---------------:|:----------:|
| Ridge Regression (lambda.min) |          22.96 |     12     |
| Lasso (lambda.min)            |          22.48 |     10     |
| Lasso (lambda.1se)            |          25.38 |     8      |
| PCR                           |          22.43 |     12     |
| PLSR                          |          22.43 |     8      |

(b) Create a new data set with `rad` as a factor. Recompute the Lasso (lambda.1se) and PLSR results. Rename the variables so you can compare results.

```{r}
Boston |> 
  dplyr::mutate(rad = as.factor(rad)) -> Boston_rf
Xrf <- model.matrix(lm(medv ~ ., data = Boston_rf))[,-1]
```

-   b.1 Lasso with `rad` as a factor.

-   b.2 Comment on the differences in the coefficients between the models with `rad` as a number and `rad` as a factor.
    For convenience you can use code similar to the following with the appropriate model names.
    (Commented so your can render the document before getting this far.)

```{r}
#| label: compare-coefs
set.seed(1234)
lassoreg_rf <- cv.glmnet(Xrf, y)

 bind_rows(
 coef(lr) |> round(3) |> as.matrix() |> as.data.frame() |> 
   tibble::rownames_to_column(),
 coef(lassoreg_rf) |> round(3) |> as.matrix() |> as.data.frame() |> 
   tibble::rownames_to_column()
 ) |> 
  mutate(model = c(rep("lasso", length(coef(lr))), rep("lasso-rf", length(coef(lassoreg_rf))))) |> 
  arrange(rowname) |>
  pivot_wider(names_from = rowname, values_from = s1) |> 
  glimpse()
 
 
pred_lassoreg_rf <- predict(lassoreg_rf, newx = Xrf, s = "lambda.1se")
pred_MSE_lassoreg_rf <- round(mean((pred_lassoreg_rf - Boston_rf$medv)^2), 
                              digits = 2)
pred_MSE_lassoreg_rf
```

Generally, we see slight decrease in the coefficients for model with `rad` as factor compared to model with `rad` as number both models shrinked the same coefficients to zero except `rad3` in the model with rad as factor which was estimated as 0.956.
Overall average median value of owner occupied home was lower in lasso model with rad as factor compared to model with rad as number.

-   b.3 PLSR with `rad` as a factor.

    ```{r}
    set.seed(1234)

    #PLSR: Rad as Factor
    plsr_rad_cv <- plsr(medv ~ ., data = Boston_rf, scale = TRUE,
                   validation = "CV")
    summary(plsr_rad_cv)

    plsr2_radf <- plsr(medv ~ ., data = Boston_rf, scale = TRUE,
                 ncomp = 13)

    msep_plsr2_radf <- MSEP(object = plsr2_radf,
          newdata = Boston_rf, ncomp = 13)
    msep_plsr2_radf$val[2]

    ```

c.  Compute both Lasso (lambda.1se) and PLSR **without `rad` in the model at all.**

    ```{r}
    set.seed(1234)
    boston_wtorad <- Boston |>
      select(-rad)

    xwtorad <- model.matrix(lm(medv ~ ., data = boston_wtorad))[,-1]
    ywtorad <- boston_wtorad$medv

    # Lasso regression: No rad
    lr_wtorad <- cv.glmnet(xwtorad, ywtorad)

    pred_lr_wtorad <- predict(lr_wtorad, newx = xwtorad, s = "lambda.1se")
    pred_MSE_lr_wtorad <- round(mean((pred_lr_wtorad - ywtorad)^2), 
                                  digits = 2)
    pred_MSE_lr_wtorad

    coef(lr_wtorad, s = "lambda.1se")

    # R2 adjusted
    lr_wtorad$glmnet.fit$dev.ratio[which(lr_wtorad$glmnet.fit$lambda == lr_wtorad$lambda.1se)]
    ```

    ```{r}
    set.seed(1234)
    # PLSR: No rad

    plsr_norad_cv <- plsr(medv ~ ., data = boston_wtorad, scale = TRUE,
                   validation = "CV")
    summary(plsr_norad_cv)

    plsr2_norad <- plsr(medv ~ ., data = boston_wtorad, scale = TRUE,
                 ncomp = 8)

    msep_plsr2_norad <- MSEP(object = plsr2_norad,
          newdata = boston_wtorad, ncomp = 8)
    msep_plsr2_norad$val[2]
    ```

d.  Summarize the results from models in b and c in the table below.
    Recommend a model and provide your rationale.

| Method                          | Prediction MSE | Predictors |
|---------------------------------|---------------:|:----------:|
| Lasso (lambda.1se) `rad`        |          25.38 |     8      |
| Lasso (lambda.1se) `rad` Factor |           25.1 |     9      |
| Lasso (lambda.1se) no `rad`     |          26.58 |     8      |
| PLSR `rad`                      |          22.43 |     8      |
| PLSR `rad` Factor               |          21.66 |     13     |
| PLSR no `rad`                   |          23.28 |     8      |

I will recommend the PLSR model with `rad` as a number.
My rationale being that `rad` is more likely a number given its definition, as the index of accessibility to radial highways, likely an ordinal measure.
Secondly the RMSEP for the PLSR with rad as numeric is 4.907 while with rad as factor is 4.807, significant difference.
In addition, the model with rad as numeric having eight components explained 73% of the variance in `medv` while the model with rad as factor having thirteen components, explained 74% of the variance in `medv`.
In conclusion despite the model with `rad` as factor having a slightly lower prediction mean square error it required more predictors to explain the variance in `medv` while the model with `rad` as number with eight components explained almost the same amount of variance with five less components.
